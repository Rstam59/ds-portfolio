{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rstam59/ds-portfolio/blob/main/TokenClassificationHuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xepGo6iRbBYp"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset('conll2003')"
      ],
      "metadata": {
        "id": "Xiu2CZfT-nNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][0]"
      ],
      "metadata": {
        "id": "jMf1dNmf-nQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_feature = raw_datasets['train'].features['ner_tags']\n",
        "ner_feature"
      ],
      "metadata": {
        "id": "vEelU_8K-nTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = ner_feature.feature.names\n",
        "label_names"
      ],
      "metadata": {
        "id": "MRnHMbq0-nVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = raw_datasets['train'][4]['tokens']\n",
        "labels = raw_datasets['train'][4]['ner_tags']\n",
        "\n",
        "line1 = ''\n",
        "line2 = ''\n",
        "\n",
        "for word, label in zip(words, labels):\n",
        "    full_label = label_names[label]\n",
        "    max_length = max(len(word), len(full_label))\n",
        "    line1 += word + ' ' * (max_length - len(word) + 1)\n",
        "    line2 += full_label + ' ' * (max_length - len(full_label) + 1)\n",
        "\n",
        "print(line1)\n",
        "print(line2)"
      ],
      "metadata": {
        "id": "7gAdvs5O-nYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = 'bert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "KKa-rKZe-na3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.is_fast"
      ],
      "metadata": {
        "id": "-o4aycWa-ndK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(raw_datasets['train'][0]['tokens'],\n",
        "                   is_split_into_words = True)\n",
        "print(inputs)\n",
        "print(inputs.tokens())\n",
        "print(inputs.word_ids())"
      ],
      "metadata": {
        "id": "irWYo41WCcQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            label = labels[word_id]\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "NTm6lkruCcSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = raw_datasets['train'][0]['ner_tags']\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels, word_ids))"
      ],
      "metadata": {
        "id": "0ekn_Se0CcUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples['tokens'],\n",
        "                        truncation = True, is_split_into_words = True)\n",
        "    all_labels = examples['ner_tags']\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs['labels'] = new_labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "b_2AY7iiEukQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched = True,\n",
        "    remove_columns = raw_datasets['train'].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "hHY7Px3QEum0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "NhULOkFHEuo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "zN3Rt9sIJCEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
        "batch['labels']"
      ],
      "metadata": {
        "id": "bKikkijpJCG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "    print(tokenized_datasets['train'][i]['labels'])"
      ],
      "metadata": {
        "id": "mTsmWfQBJyUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "DFHoljVGJyXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "Z06DpxvROQgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load('seqeval')"
      ],
      "metadata": {
        "id": "xFIkxS12L2Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = raw_datasets['train'][0]['ner_tags']\n",
        "labels = [label_names[i] for i in labels]\n",
        "labels"
      ],
      "metadata": {
        "id": "kGu7MtBAL2EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = labels.copy()\n",
        "predictions[2] = 'O'\n",
        "metric.compute(predictions = [predictions], references = [labels])"
      ],
      "metadata": {
        "id": "7agGN6ljMsUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis = -1)\n",
        "\n",
        "    true_labels = [[label_names[l] for l in label if l != - 100]\n",
        "                   for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions = true_predictions,\n",
        "                                 references = true_labels)\n",
        "    return {\n",
        "        'precision': all_metrics['overall_precision'],\n",
        "        'recall': all_metrics['overall_recall'],\n",
        "        'f1': all_metrics['overall_f1'],\n",
        "        'accuracy': all_metrics['overall_accuracy'],\n",
        "    }"
      ],
      "metadata": {
        "id": "Bg8oByhxJyZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ],
      "metadata": {
        "id": "tv_LO6fEPPz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label = id2label,\n",
        "    label2id = label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "YL-1dzAOPP3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "-89h6usNPe-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    'bert-finetuned-ner',\n",
        "    eval_strategy= 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    num_train_epochs = 3,\n",
        "    weight_decay = 0.01,\n",
        "    push_to_hub = True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-DeDFYhiPfAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "G5XmxGJ6U4yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = args,\n",
        "    train_dataset = tokenized_datasets['train'],\n",
        "    eval_dataset = tokenized_datasets['validation'],\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics = compute_metrics,\n",
        "    processing_class = tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RgXVcHIHPfCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w66uyxKAWl8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message = 'Training complete')"
      ],
      "metadata": {
        "id": "N2viMt__RErS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2YLJomJtREt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiNit3qs6gDG"
      },
      "source": [
        "#A custom training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets['train'],\n",
        "    shuffle = True,\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = 8\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets['validation'],\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = 8\n",
        ")"
      ],
      "metadata": {
        "id": "wLK6Zy-sWnGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label = id2label,\n",
        "    label2id = label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "B86KWZccWnCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)"
      ],
      "metadata": {
        "id": "rVzjMGhfWm-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "MSlKEGv1Wm6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    'linear',\n",
        "    optimizer = optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps = num_training_steps\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "1OuJbYlfWm2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = 'bert-finetuned-ner-accelerate'\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ],
      "metadata": {
        "id": "SY2QMWZEWmxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(repo_name, exist_ok=True)"
      ],
      "metadata": {
        "id": "QCCKm9CxcHb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'bert-finetuned-ner-accelerate'\n",
        "repo = Repository(output_dir, clone_from = repo_name)"
      ],
      "metadata": {
        "id": "WYgVA-X7Wmr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.detach().cpu().clone().numpy()\n",
        "    labels = labels.detach().cpu().clone().numpy()\n",
        "\n",
        "    true_labels = [[label_names[l] for l in label if l != -100]\n",
        "                    for label in labels]\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    return true_labels, true_predictions"
      ],
      "metadata": {
        "id": "-T1XJCMObf9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "\n",
        "        predictions = outputs.logits.argmax(dim = -1)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        predictions = accelerator.pad_across_processes(predictions, dim = 1, pad_index = -100)\n",
        "        labels = accelerator.pad_across_processes(labels, dim = 1, pad_index = -100)\n",
        "\n",
        "        predictions_gathered = accelerator.gather(predictions)\n",
        "        labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
        "        metric.add_batch(predictions = true_predictions, references = true_labels)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(\n",
        "        f\"epoch {epoch}:\",\n",
        "        {\n",
        "            key: results[f\"overall_{key}\"]\n",
        "            for key in ['precision', 'recall', 'f1', 'accuracy']\n",
        "        }\n",
        "    )\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message = f\"Training in progress epoch {epoch}\", blocking = False\n",
        "        )"
      ],
      "metadata": {
        "id": "tcGA8u6ubgAB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPqZrpL0a1yebBBxkp2gOTS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}