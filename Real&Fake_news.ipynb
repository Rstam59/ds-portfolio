{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMB+TKE4qYuvMvdnIrBr75k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rstam59/ds-portfolio/blob/main/Real%26Fake_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsftui9IlYYW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory for kaggle and move the file\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Install Kaggle CLI tool\n",
        "!pip install -q kaggle\n"
      ],
      "metadata": {
        "id": "-6jacJTolxd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset using Kaggle CLI\n",
        "!kaggle datasets download -d razanaqvi14/real-and-fake-news\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip real-and-fake-news.zip\n"
      ],
      "metadata": {
        "id": "CmIyAlgElz2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "real_df = pd.read_csv(\"True.csv\")\n",
        "fake_df = pd.read_csv(\"Fake.csv\")\n",
        "\n",
        "\n",
        "real_df['label'] = 1\n",
        "fake_df['label'] = 0\n",
        "\n",
        "df = pd.concat([real_df, fake_df], ignore_index=True)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hdIOxF5Wl9Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(data=df, x='label')\n",
        "plt.title(\"Label Distribution (0: Fake, 1: Real)\")\n",
        "plt.show()\n",
        "\n",
        "print(df['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "AqhbhPmqmF6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "-KasVW3ipeZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_len'] = df['text'].apply(lambda x: len(x.split()))\n",
        "df['title_len'] = df['title'].apply(lambda x: len(x.split()))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(df['text_len'], bins=50, kde=True)\n",
        "plt.title(\"Text Length Distribution\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(df['title_len'], bins=50, kde=True)\n",
        "plt.title(\"Title Length Distribution\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JT-nUlWPNZmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique subjects:\", df['subject'].unique())\n",
        "print(df['subject'].value_counts())\n"
      ],
      "metadata": {
        "id": "zvDJCtNhNoQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join all fake and real text\n",
        "fake_text = \" \".join(df[df['label']==0]['text'].astype(str).tolist())\n",
        "real_text = \" \".join(df[df['label']==1]['text'].astype(str).tolist())\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "WordCloud(max_words=200, background_color=\"white\").generate(fake_text)\n",
        "plt.imshow(WordCloud().generate(fake_text), interpolation='bilinear')\n",
        "plt.title(\"Fake News\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "WordCloud(max_words=200, background_color=\"white\").generate(real_text)\n",
        "plt.imshow(WordCloud().generate(real_text), interpolation='bilinear')\n",
        "plt.title(\"Real News\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cJ0IooBzNxiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorization"
      ],
      "metadata": {
        "id": "G6bBLrW1N5E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "Pdz2gtrUbhCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df['content'] = df['title'] + ' ' + df['text']\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "df['clean_content'] = df['content'].apply(clean_text)\n",
        "df = df.drop_duplicates(subset='clean_content')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['clean_content'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
        ")"
      ],
      "metadata": {
        "id": "QCQnNfIIPB3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "MAX_SEQUENCE_LENGTH = 1200\n",
        "\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens=MAX_VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "\n",
        "vectorizer.adapt(X_train.values)\n"
      ],
      "metadata": {
        "id": "mllO5E4UQYY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val.values, y_val.values))\n",
        "\n",
        "train_ds = train_ds.shuffle(1024).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "3UFr2kPSQuvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Input\n",
        "\n",
        "# Constants\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Input(shape=(1,), dtype=tf.string),\n",
        "    vectorizer,\n",
        "    Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "6tl2xMpnTWOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "CMf9b3BbWhyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(val_ds)\n",
        "print(f\"\\nValidation Accuracy: {acc:.4f}\")\n",
        "print(f\"Validation Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "O_96FEbcW9eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "id": "w5e9ANStWG3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(val_ds)\n",
        "y_pred = (preds > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "_Rg5N6IvYDT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Fake\", \"Real\"], yticklabels=[\"Fake\", \"Real\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uXYfhi80Wo-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sample(text):\n",
        "    text = clean_text(text)\n",
        "    text = tf.constant([text])\n",
        "    pred = model.predict(text)[0][0]\n",
        "    label = \"Real\" if pred > 0.5 else \"Fake\"\n",
        "    print(f\"Prediction: {label} ({pred:.2f})\")\n",
        "\n",
        "\n",
        "predict_sample(\"President signs a new healthcare bill into law.\")\n",
        "predict_sample(\"BREAKING: Aliens seen entering the White House.\")\n",
        "predict_sample(\"The quick brown fox jumps over the lazy dog.\")\n",
        "predict_sample('state dept subpoenaed documents from clinton foundation report washington reuters us state department investigators last year issued a subpoena to the bill hillary and chelsea clinton foundation seeking documents about projects run by the charity that may have required us government approval when hillary clinton was secretary of state the washington post reported on thursday a us official said the matter was being investigated by the inspector general the state departments internal watchdog citing unnamed sources for the report the post said the subpoena issued in the fall also asked for records related to senior clinton aide huma abedin who for six months in simultaneously worked for several employers including the state department the foundation and clintons personal office the report follows a reuters investigation last year that found the clinton foundations flagship health project did not submit new or increased payments from at least seven foreign governments to the state department for review in breach of the ethics agreement clinton signed with the incoming obama administration in order to become secretary of state clinton who is running for the democratic nomination in the nov presidential election has been criticized for using a private email account hosted on a private computer while secretary of state from to a matter the fbi is investigating spokesmen for clintons campaign and the clinton foundation and a lawyer for abedin did not immediately respond to reuters requests for comment a spokesman for the inspector general also declined to comment the post quoted an unnamed foundation representative as saying the initial document request had been narrowed by investigators and that the foundation was not the focus of the probe it said there was no indication that the investigators were looking at clinton the full scope and status of the inquiry conducted by the state departments inspector general were not clear from the material correspondence reviewed by the washington post the paper said sources familiar with investigations into the controversy surrounding clintons private email server said they had no reason to believe any government agency was conducting any kind of inquiry into possible criminal violations related to the former secretary of state')"
      ],
      "metadata": {
        "id": "uil_wgO0W5hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.iloc[0]"
      ],
      "metadata": {
        "id": "vx5xjOdAYd0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val.iloc[0]"
      ],
      "metadata": {
        "id": "dKSOJ8Zxaq24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "VqlZ80BWndFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets"
      ],
      "metadata": {
        "id": "b4bD35oWITTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_ds = Dataset.from_pandas(pd.DataFrame({'text': X_train, 'label': y_train}))\n",
        "val_ds = Dataset.from_pandas(pd.DataFrame({'text': X_val, 'label': y_val}))"
      ],
      "metadata": {
        "id": "ZqMuOk7YCIMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "n2ckegdAClcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example['text'], truncation=True, padding='longest')\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(tokenize, batched = True)\n",
        "val_ds = val_ds.map(tokenize, batched = True)"
      ],
      "metadata": {
        "id": "bagGE1ttEToK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.set_format('torch', columns = ['input_ids', 'attention_mask', 'label'])\n",
        "val_ds.set_format('torch', columns = ['input_ids', 'attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "IBzLtTXQE1h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.remove_columns(['text', '__index_level_0__'])\n",
        "val_ds = val_ds.remove_columns(['text', '__index_level_0__'])"
      ],
      "metadata": {
        "id": "5cpN3LhLFT6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    fp16=True,  # Enable mixed precision\n",
        ")\n"
      ],
      "metadata": {
        "id": "OGzYXxdOFzte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'f1': f1_score(labels, preds, average='weighted')\n",
        "    }\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset = val_ds,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "khaFv-YVGwxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"fake-news-bert-model\")\n",
        "tokenizer.save_pretrained(\"fake-news-bert-model\")\n"
      ],
      "metadata": {
        "id": "rTTkvTHdHcPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lime\n"
      ],
      "metadata": {
        "id": "8ncVt0aGQj3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import torch\n",
        "\n",
        "# Wrap the trained model into a pipeline\n",
        "pipeline = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=None,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n"
      ],
      "metadata": {
        "id": "AF0tGBlxQpem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = LimeTextExplainer(class_names=[\"Fake\", \"Real\"])\n",
        "\n",
        "# Example input\n",
        "text = \"President Trump Said I love Unicorns and they are 100% Real!\"\n",
        "\n",
        "# Explain\n",
        "exp = explainer.explain_instance(\n",
        "    text_instance=text,\n",
        "    classifier_fn=lambda x: np.array([[score['score'] for score in sorted(p, key=lambda y: y['label'])] for p in pipeline(x)]),\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "# Show Explanation\n",
        "exp.show_in_notebook(text=True)"
      ],
      "metadata": {
        "id": "fe5oB3ZCQ-jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "train_loader = DataLoader(train_ds, shuffle = True, batch_size = 16)\n",
        "eval_loader = DataLoader(val_ds, batch_size = 16)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
        "\n",
        "model, optimizer, train_loader, eval_loader = accelerator.prepare(model,\n",
        "                                                                  optimizer, train_loader, eval_loader)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    loop = tqdm(train_loader, desc = f\"Epoch {epoch}\")\n",
        "    for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "        batch['labels'] = batch.pop('label')\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "UYD4VVsHTZMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        batch['labels'] = batch.pop('label')\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim = -1).detach().cpu().numpy()\n",
        "        labels = batch['labels'].detach().cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Accuracy: {acc}\")"
      ],
      "metadata": {
        "id": "cyWf__tyT7eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbstripout\n"
      ],
      "metadata": {
        "id": "TUaM6fMLbD9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Path to your current notebook — this is how Colab stores it\n",
        "notebook_path = \"/content/Real&Fake_news.ipynb\"  # ⬅️ Change this to match your filename\n",
        "\n",
        "# Load the notebook\n",
        "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "# OPTIONAL: keep outputs if you want them visible on GitHub\n",
        "KEEP_OUTPUT = True\n",
        "\n",
        "# Clean the notebook\n",
        "for cell in nb.cells:\n",
        "    if not KEEP_OUTPUT:\n",
        "        cell[\"outputs\"] = []\n",
        "        cell[\"execution_count\"] = None\n",
        "\n",
        "# Remove broken widget metadata\n",
        "nb.metadata.pop(\"widgets\", None)\n",
        "\n",
        "# Save cleaned version\n",
        "cleaned_path = \"/content/cleaned_notebook.ipynb\"\n",
        "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(f\"✅ Cleaned notebook saved to: {cleaned_path}\")\n"
      ],
      "metadata": {
        "id": "TBKWl5zBboEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkS0GWmjb0u9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}