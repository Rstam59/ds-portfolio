{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDy3lVNahKz5Vezu0roKDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rstam59/ds-portfolio/blob/main/NLP_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "40oXelXKw-i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
        "    as_supervised=True\n",
        ")\n",
        "tf.random.set_seed(42)\n",
        "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
        "test_set = raw_test_set.batch(32).prefetch(1)"
      ],
      "metadata": {
        "id": "uZrqdBDRxDjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for review, label in raw_train_set.take(4):\n",
        "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
        "    print(\"Label:\", label.numpy())"
      ],
      "metadata": {
        "id": "-CopB7dpYnnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size,\n",
        "                                                   standardize=\"lower_and_strip_punctuation\",\n",
        "                                                   split = 'whitespace')\n",
        "\n",
        "text_vec_layer.adapt(train_set.map(lambda review, label: review))"
      ],
      "metadata": {
        "id": "1QwlnZyoYnq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(map(str, text_vec_layer.get_vocabulary()[:20]))"
      ],
      "metadata": {
        "id": "93WpSFnbYnte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer(['it was a great movie',\n",
        "                'it was'])"
      ],
      "metadata": {
        "id": "qgXfwm-3YnwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64)\n",
        "# embed_layer(text_vec_layer(['it was a great movie']))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Wmc2YuboYnyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, mask_zero = True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "XUDBryjRbGFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = 'binary_crossentropy',\n",
        "    optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(train_set, validation_data=valid_set, epochs=2)"
      ],
      "metadata": {
        "id": "UFQpi3GPbGO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights = model.layers[1].get_weights()[0]"
      ],
      "metadata": {
        "id": "3Fx1cel3hVSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"embeddings.tsv\", embedding_weights, delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "pDi-CqnD17Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = text_vec_layer.get_vocabulary()\n",
        "\n",
        "with open(\"metadata.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for word in vocab:\n",
        "        word = word if word.strip() != \"\" else \"<PAD>\"\n",
        "        f.write(f\"{word}\\n\")\n"
      ],
      "metadata": {
        "id": "czM6dDN717J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ],
      "metadata": {
        "id": "SLB17Rbs8JnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "metadata": {
        "id": "wixJKtiJ8JqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "id": "WeGiprLgpN19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(sorted(set(shakespeare_text.lower())))"
      ],
      "metadata": {
        "id": "ySmfKm9Opkpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split = 'character',\n",
        "                                                   standardize = 'lower')\n",
        "\n",
        "text_vec_layer.adapt(shakespeare_text)\n",
        "encoded = text_vec_layer([shakespeare_text][0])"
      ],
      "metadata": {
        "id": "n0SBkzk8pksp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2\n",
        "vocab_size = text_vec_layer.vocabulary_size() - 2\n",
        "dataset_size = len(encoded)"
      ],
      "metadata": {
        "id": "IU9be1qApkv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input ->  Before we proceed any furthe\n",
        "# output -> efore we proceed any further"
      ],
      "metadata": {
        "id": "lo4f9oJglJIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, seed = None, shuffle = False, batch_size = 32):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift = 1, drop_remainder= True)\n",
        "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(100_000, seed = seed)\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ],
      "metadata": {
        "id": "0HZqJYiapk0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7O7TRvlWplBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VodnWaEGplEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "sequence = tf.constant([10, 20, 30, 40, 50, 60, 70, 80], dtype=tf.int32)\n",
        "length = 4  # input length\n",
        "batch_size = 2\n"
      ],
      "metadata": {
        "id": "USUuwvve__tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "print(\"Step 1: from_tensor_slices\")\n",
        "for x in ds:\n",
        "    print(x.numpy())\n"
      ],
      "metadata": {
        "id": "YuKdjv-w__9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "print(\"\\nStep 2: window\")\n",
        "for i, window_ds in enumerate(ds):\n",
        "    print(f\"Window {i+1}:\", [int(el) for el in window_ds.as_numpy_iterator()])\n"
      ],
      "metadata": {
        "id": "FtCnLJffERCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.flat_map(lambda window: window.batch(length + 1))\n",
        "print(\"\\nStep 3: flat_map + batch\")\n",
        "for x in ds:\n",
        "    print(x.numpy())\n"
      ],
      "metadata": {
        "id": "BQdKkNoVEV7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.batch(batch_size)\n",
        "print(\"\\nStep 4: batching\")\n",
        "for batch in ds:\n",
        "    print(batch.numpy())\n"
      ],
      "metadata": {
        "id": "dM287YlHEl_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
        "print(\"\\nStep 5: split into input and target\")\n",
        "for input_seq, target_seq in ds:\n",
        "    print(\"Input:\", input_seq.numpy())\n",
        "    print(\"Target:\", target_seq.numpy())\n"
      ],
      "metadata": {
        "id": "j7KUlgeoEvdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length = 100\n",
        "tf.random.set_seed(42)\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
        "                       seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ],
      "metadata": {
        "id": "mJTLGw9ZEy0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "uGJXfWHWH3kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input ->  Before we proceed any furthe\n",
        "# output -> efore we proceed any further"
      ],
      "metadata": {
        "id": "LsERc6sjqQDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences = True),\n",
        "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_shakespeare_model.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    train_set,\n",
        "    validation_data=valid_set,\n",
        "    epochs=1,\n",
        "    callbacks=[model_ckpt]\n",
        ")"
      ],
      "metadata": {
        "id": "9aULvPnFu-nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
        "    model\n",
        "])"
      ],
      "metadata": {
        "id": "mlYLVVuBkGUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict(tf.constant(['To be or not to b']))[0, -1]\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "id": "osGWsHRGkIlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Fake Shakespearean text"
      ],
      "metadata": {
        "id": "GeZagfSew6bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = tf.math.log([[0.5, 0.3, 0.2]])\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas, num_samples=8)"
      ],
      "metadata": {
        "id": "8xw18sPkw6gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature = 1):\n",
        "    text = tf.constant([text])\n",
        "    y_proba = shakespeare_model.predict(text)[0, -1:]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples = 1)[0, 0]\n",
        "    return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "-Uo6-bnb7bmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extent_text(text, chars = 50, temperature = 1):\n",
        "    for _ in range(chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "sdeo_pAm_xf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extent_text('to be or not to', chars = 100, temperature = 1)"
      ],
      "metadata": {
        "id": "obPHAxMvBKMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature):\n",
        "    text = tf.constant([text])\n",
        "    y_proba = shakespeare_model.predict(text)[0, -1:]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples = 1)[0, 0]\n",
        "    return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "gDiZRwdIl_pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extent_text(text, chars = 50, temperature = 1):\n",
        "    for _ in range(chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "kc9Jjn1unI3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extent_text('to be or not to b', chars = 100, temperature = 1)"
      ],
      "metadata": {
        "id": "EBhtHy_boO2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extent_text('to be or not to b', chars = 100, temperature = 0.1)"
      ],
      "metadata": {
        "id": "XoWWKactpR1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extent_text('to be or not to b', chars = 50, temperature = 0.001)"
      ],
      "metadata": {
        "id": "Q_y7TImFpzd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature):\n",
        "    text = tf.constant([text])\n",
        "    y_proba = shakespeare_model.predict(text)[0, -1:]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
        "    return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "WDKb0haxtwSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extent_text(text, chars = 50, temperature = 1):\n",
        "    for _ in range(chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "BMDVC8HwumOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extent_text('to be or not to be', temperature = 1)"
      ],
      "metadata": {
        "id": "vEVq0h5utxMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extent_text('to be or not to be', chars= 10, temperature = 100)"
      ],
      "metadata": {
        "id": "Y-1FDow8xbX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWMa--QNwOu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Encoder–Decoder Network for Neural Machine Translation"
      ],
      "metadata": {
        "id": "Omxt4_a0P2jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True)\n",
        "\n",
        "# Final corrected path\n",
        "spa_txt_path = Path(path).parent / \"spa-eng_extracted\" / \"spa-eng\" / \"spa.txt\"\n",
        "\n",
        "# Read the file\n",
        "text = spa_txt_path.read_text(encoding='utf-8')\n",
        "print(text[:500])  # Print first 500 characters as a quick check\n"
      ],
      "metadata": {
        "id": "Tafas0_JQB6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ],
      "metadata": {
        "id": "t_Jhjg2OqsSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(sentences_en[i], '=>', sentences_es[i])"
      ],
      "metadata": {
        "id": "CT1aBtso2yHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
      ],
      "metadata": {
        "id": "_8HaxCr82yQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "Xy3X3wUx2yS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "qw-iTep_7NE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f'startofseq {s}' for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[100_000:]])"
      ],
      "metadata": {
        "id": "QB1SY7MuN9yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "DL8mKohIOjON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "HpPanRJwRGzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state = True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
      ],
      "metadata": {
        "id": "ex69srznS8WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences = True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state = encoder_state)"
      ],
      "metadata": {
        "id": "0IqklQ7WTARv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs],\n",
        "                       outputs = [Y_proba])\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'nadam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs = 3,\n",
        "          validation_data = ((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "OOLASo4HTEC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence_en):\n",
        "    translation = ''\n",
        "    X = tf.constant([sentence_en])\n",
        "\n",
        "    for word_idx in range(max_length):\n",
        "        X_dec = tf.constant(['startofseq' + translation])\n",
        "        y_proba = model.predict((X, X_dec))[0, word_idx]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == 'endofseq':\n",
        "            break\n",
        "        translation += ' ' + predicted_word\n",
        "\n",
        "    return translation.strip()"
      ],
      "metadata": {
        "id": "Ul7f6_JkVh1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i like soccer and also going to the beach')"
      ],
      "metadata": {
        "id": "L5Y_VZYKVsAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i like soccer and also going to the beach')"
      ],
      "metadata": {
        "id": "cS3_TCsCKp2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional RNNs"
      ],
      "metadata": {
        "id": "DayYLy_fU47u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True)\n",
        ")"
      ],
      "metadata": {
        "id": "v4iVamY5VDTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "# encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "#                  tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)"
      ],
      "metadata": {
        "id": "VwoQY7njVI9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatenateStates(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def call(self, encoder_state):\n",
        "        return [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "                tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n",
        "\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "concat_states = ConcatenateStates()\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "Yzil99dBAXlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"nadam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "AG_qzWRVSGb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i like cats and dogs')"
      ],
      "metadata": {
        "id": "zOqDLsqqT-c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beam Search"
      ],
      "metadata": {
        "id": "P9Sg2OwvArBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = tf.constant([sentence_en])  # encoder input\n",
        "    X_dec = tf.constant([\"startofseq\"])  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = tf.constant([sentence_en])  # encoder input\n",
        "            X_dec = tf.constant([\"startofseq \" + translation])  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()\n"
      ],
      "metadata": {
        "id": "etArHX30A8KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en = \"i like cats and dogs\"\n",
        "translate(sentence_en)\n"
      ],
      "metadata": {
        "id": "EAYMaEHcBy8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(sentence_en, beam_width=3, verbose=True)\n"
      ],
      "metadata": {
        "id": "JWCm7ePuCzOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanisms"
      ],
      "metadata": {
        "id": "ctSfo--oC82y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "kfkk2h8TVupJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size)\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "yBHKY__lWdxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences = True, return_state=True)\n",
        ")\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "nky8lu0sXtIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences = True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state = encoder_state)"
      ],
      "metadata": {
        "id": "DBq6d70wYoOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "E0JXQeQ6Y4rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i love soccer and also going to the beach')"
      ],
      "metadata": {
        "id": "nIqOem9-aj-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xkCs_3Q-aQrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = tf.keras.layers.MultiHeadAttention(num_heads = 1, key_dim = embed_size)\n",
        "attention_outputs = attention_layer(\n",
        "    query = decoder_outputs,\n",
        "    value = encoder_outputs,\n",
        "    key = encoder_outputs)\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "NdzgLnyQXBVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en = \"I love soccer and also going to the beach\"\n",
        "translate(sentence_en)\n"
      ],
      "metadata": {
        "id": "pTsRUFJxZI_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Is All You Need: The Transformer Architecture\n",
        "# Positional encodings"
      ],
      "metadata": {
        "id": "dCn6qGpz9Lvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length),\n",
        "                           2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]\n",
        "\n"
      ],
      "metadata": {
        "id": "TSgMVRNLxYdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)\n"
      ],
      "metadata": {
        "id": "_pmb5b7Y0Asg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# extra code – this cells generates and saves Figure 16–9\n",
        "figure_max_length = 201\n",
        "figure_embed_size = 512\n",
        "pos_emb = PositionalEncoding(figure_max_length, figure_embed_size)\n",
        "zeros = np.zeros((1, figure_max_length, figure_embed_size), np.float32)\n",
        "P = pos_emb(zeros)[0].numpy()\n",
        "i1, i2, crop_i = 100, 101, 150\n",
        "p1, p2, p3 = 22, 60, 35\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
        "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
        "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
        "ax1.plot(p3, P[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
        "ax1.plot(P[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
        "ax1.plot(P[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
        "ax1.plot([p1, p2], [P[p1, i1], P[p2, i1]], \"bo\")\n",
        "ax1.plot([p1, p2], [P[p1, i2], P[p2, i2]], \"ro\")\n",
        "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
        "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.hlines(0, 0, figure_max_length - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
        "ax1.axis([0, figure_max_length - 1, -1, 1])\n",
        "ax2.imshow(P.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
        "ax2.hlines(i1, 0, figure_max_length - 1, color=\"b\", linewidth=3)\n",
        "cheat = 2  # need to raise the red line a bit, or else it hides the blue one\n",
        "ax2.hlines(i2+cheat, 0, figure_max_length - 1, color=\"r\", linewidth=3)\n",
        "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
        "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
        "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
        "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
        "ax2.axis([0, figure_max_length - 1, 0, crop_i])\n",
        "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
        "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sJcPIo000DAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model"
      ],
      "metadata": {
        "id": "WCokxmQI0FD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 50\n",
        "embed_size = 128\n",
        "num_heads = 5\n",
        "ff_dim = 512\n",
        "\n",
        "#Input layers\n",
        "encoder_inputs = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"encoder_inputs\")\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"decoder_inputs\")\n",
        "\n",
        "\n",
        "\n",
        "#Embedding layer\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "\n",
        "#Positional embeddings\n",
        "pos_embedding_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "positions_encoder = tf.keras.layers.Lambda(lambda x: tf.range(start = 0, limit = tf.shape(x)[1], delta = 1))(encoder_inputs)\n",
        "positions_decoder = tf.keras.layers.Lambda(lambda x: tf.range(start = 0, limit = tf.shape(x)[1], delta = 1))(decoder_inputs)\n",
        "pos_embed_enc = pos_embedding_layer(positions_encoder)\n",
        "pos_embed_dec = pos_embedding_layer(positions_decoder)\n",
        "\n",
        "#Add tokens and positional embeddings\n",
        "encoder_embed = encoder_embeddings + pos_embed_enc\n",
        "decoder_embed = decoder_embeddings + pos_embed_dec\n",
        "\n",
        "\n",
        "#Encoder self-attention\n",
        "encoder_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_size)(encoder_embed, encoder_embed)\n",
        "encoder_attention = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(encoder_embed + encoder_attention)\n",
        "\n",
        "#Encoder Feed-forward\n",
        "encoder_ff = tf.keras.layers.Dense(ff_dim, activation = 'relu')(encoder_attention)\n",
        "encoder_ff = tf.keras.layers.Dense(embed_size)(encoder_ff)\n",
        "encoder_outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(encoder_attention + encoder_ff)\n",
        "\n",
        "\n",
        "#Decoder self-attention\n",
        "causal_mask = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0)\n",
        ")(decoder_inputs)\n",
        "decoder_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim=embed_size)(decoder_embed, decoder_embed, attention_mask = causal_mask)\n",
        "decoder_attention = tf.keras.layers.LayerNormalization(epsilon= 1e-6)(decoder_embed + decoder_attention)\n",
        "\n",
        "#Encoder-Decoder Cross Attention\n",
        "cross_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_size)(decoder_attention, encoder_outputs, encoder_outputs)\n",
        "decoder_cross = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(decoder_attention + cross_attention)\n",
        "\n",
        "\n",
        "\n",
        "#Decoder feed-forward\n",
        "decoder_ff = tf.keras.layers.Dense(ff_dim, activation = 'relu')(decoder_cross)\n",
        "decoder_ff = tf.keras.layers.Dense(embed_size)(decoder_ff)\n",
        "decoder_outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(decoder_cross + decoder_ff)\n",
        "\n",
        "\n",
        "#Final output layer\n",
        "output_logits = tf.keras.layers.Dense(vocab_size, activation = 'softmax')(decoder_outputs)\n",
        "\n",
        "#Model\n",
        "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], output_logits)"
      ],
      "metadata": {
        "id": "5FTA_Y-zqVQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"nadam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# transformer.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "#           validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "4DhMa6Rh-kvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "max_length = 50\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
      ],
      "metadata": {
        "id": "FziJXhjWCSU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_train).numpy(), padding=\"post\", maxlen=max_length\n",
        ")\n",
        "X_train_dec_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_es(X_train_dec).numpy(), padding=\"post\", maxlen=max_length\n",
        ")\n",
        "\n",
        "X_valid_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_valid).numpy(), padding=\"post\", maxlen=max_length\n",
        ")\n",
        "X_valid_dec_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_es(X_valid_dec).numpy(), padding=\"post\", maxlen=max_length\n",
        ")\n",
        "\n",
        "X_train_padded = tf.constant(X_train_padded)\n",
        "X_train_dec_padded = tf.constant(X_train_dec_padded)\n",
        "X_valid_padded = tf.constant(X_valid_padded)\n",
        "X_valid_dec_padded = tf.constant(X_valid_dec_padded)\n",
        "\n",
        "transformer.fit(\n",
        "    (X_train_padded, X_train_dec_padded),\n",
        "    Y_train,\n",
        "    epochs=3,\n",
        "    validation_data=((X_valid_padded, X_valid_dec_padded), Y_valid),\n",
        ")"
      ],
      "metadata": {
        "id": "HC4dhXFi--mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def translate(sentence_en):\n",
        "    # Tokenize and pad encoder input\n",
        "    X = text_vec_layer_en(tf.constant([sentence_en]))\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(X.numpy(), padding=\"post\", maxlen=max_length)\n",
        "\n",
        "    # Start token\n",
        "    start_token = text_vec_layer_es([ 'startofseq'])[0][0]\n",
        "    end_token = text_vec_layer_es(['endofseq'])[0][0]\n",
        "\n",
        "    # Decoder input initialized with just the start token\n",
        "    decoder_input = [start_token]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        decoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [decoder_input], maxlen=max_length, padding=\"post\"\n",
        "        )\n",
        "\n",
        "        y_proba = transformer.predict((X, decoder_input_padded), verbose=0)[0, len(decoder_input)-1]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "\n",
        "        if predicted_word_id == end_token:\n",
        "            break\n",
        "\n",
        "        decoder_input.append(predicted_word_id)\n",
        "\n",
        "    # Map tokens back to words\n",
        "    vocab = text_vec_layer_es.get_vocabulary()\n",
        "    translated_words = [vocab[token] for token in decoder_input[1:]]  # skip start token\n",
        "\n",
        "    return ' '.join(translated_words)\n"
      ],
      "metadata": {
        "id": "hxW4_XuRBQLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(\"i like soccer and also going to the beach\"))\n"
      ],
      "metadata": {
        "id": "vlUnWLozBtE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPrzkKbUpHuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}